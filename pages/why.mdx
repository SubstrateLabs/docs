import { SmallCards, SmallCard } from "../components/SmallCards";
import { Callout, Tabs } from "nextra/components";
import Link from "next/link";
import Image from "next/image";
import {
  ComputeText,
  ComputeJSON,
  GenerateImage,
  GenerateSpeech,
  UpscaleImage,
  QueryVectorStore,
} from "@/components/nodes";
import Diagram from "@/components/diagrams/rag-why";

### Compound AI

Combining multiple model outputs is an effective approach for improving the results of AI applications. A popular example of this is [RAG](/guides/rag-intro), in which search is used to augment the context of a language model. It often looks something like this:

<Diagram />

Tools like [LangChain](https://www.langchain.com/) are directionally right because they encourage pipelining. But they offer unwieldy (and often unnecessary) abstractions, and produce fundamentally inefficient programs. Read our [blog post](https://blog.substrate.run/blog/langchain) on the cognitive overhead of using LangChain, compared to Substrate.

### Faster execution

Whether you use LangChain, or wire together multiple inference API requests yourself, what's happening under the hood looks something like this:

<Image src="/without-substrate.png" width={800} height={800} />

With Substrate, it becomes this:

<Image src="/with-substrate.png" width={800} height={800} />

### Better code

With Substrate, the code to describe your system also becomes dramatically more legible.

Below is a code example from [Together AI](https://docs.together.ai/docs/mixture-of-agents#advanced-moa-example), next to the same example using Substrate. The example implements the "Mixture of Agents" technique, in which multiple LLM responses are combined repeatedly to produce better outputs.

The code on the left includes custom business logic to handle rate limits and parallellize LLM calls in batches. The code is verbose, hard to follow, and the parallelism could be improved.

With Substrate, you don't need to think about parallelizing your code or handling rate limits. Parallelization happens automatically when Substrate runs your workload. Instead of rate limits, Substrate uses a concurrency limit – the number of nodes that you can run in parallel. This is much easier to work with than rate limits – Substrate automatically manages scheduling nodes according to your concurrency limit.

<Tabs items={['Without Substrate', 'With Substrate' ]}>

<Tabs.Tab>

<CH.Code>

```python
reference_models = [
    "Qwen/Qwen2-72B-Instruct",
    "Qwen/Qwen1.5-72B-Chat",
    "mistralai/Mixtral-8x22B-Instruct-v0.1",
    "databricks/dbrx-instruct",
]
aggregator_model = "mistralai/Mixtral-8x22B-Instruct-v0.1"
layers = 3

async def run_llm(model, prev_response=None):
    # mark
    for sleep_time in [1, 2, 4]:
        try:
            messages = (
                [
                    {
                        "role": "system",
                        "content": getFinalSystemPrompt(
                            aggreagator_system_prompt, prev_response
                        ),
                    },
                    {"role": "user", "content": user_prompt},
                ]
                if prev_response
                else [{"role": "user", "content": user_prompt}]
            )
            response = await async_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.7,
                max_tokens=512,
            )
            print("Model: ", model)
            break
        # mark
        except together.error.RateLimitError as e:
            print(e)
            await asyncio.sleep(sleep_time)
    return response.choices[0].message.content

async def main():
    # mark
    results = await asyncio.gather(*[run_llm(model) for model in reference_models])
    # mark
    for _ in range(1, layers - 1):
        # mark
        results = await asyncio.gather(
            # mark
            *[run_llm(model, prev_response=results) for model in reference_models]
        # mark
        )

    finalStream = client.chat.completions.create(
        model=aggregator_model,
        messages=[
            {
                "role": "system",
                "content": getFinalSystemPrompt(aggreagator_system_prompt, results),
            },
            {"role": "user", "content": user_prompt},
        ],
        stream=True,
    )
    for chunk in finalStream:
        print(chunk.choices[0].delta.content or "", end="", flush=True)
```

</CH.Code>

</Tabs.Tab>

<Tabs.Tab>

<CH.Code>

```python
models = [
    "Mistral7BInstruct",
    "Mixtral8x7BInstruct",
    "Llama3Instruct8B",
    "Llama3Instruct70B",
]
decider = "Llama3Instruct70B"
num_layers = 3

def get_mixture(q, prev=None):
    prompt = sb.concat(aggregate, "\n\nquestion: ", q, "\n\nprevious:\n\n", prev) if prev else q
    return Box(value=[ComputeText(prompt=prompt, model=m).future.text for m in models])

def main():
    layers = [get_mixture(question)]

    def last_layer():
        return sb.jq(layers[-1].future.value, 'to_entries | map(((.key + 1) | tostring) + ". " + .value) | join("\n")')

    for _ in range(num_layers - 1):
        layers.append(get_mixture(question, last_layer()))

    final = ComputeText(prompt=sb.concat(aggregate, "\n\n", last_layer()), model=decider)

    box = Box(value={"layers": [l.future.value for l in layers], "final": final.future.text})
    res = substrate.run(box)
```

</CH.Code>

</Tabs.Tab>

</Tabs>
